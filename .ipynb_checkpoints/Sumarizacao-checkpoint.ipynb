{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e9a6a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.topcoder.com/thrive/articles/text-summarization-in-nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d52202d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/yansym/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution -andas (/home/yansym/anaconda3/lib/python3.9/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -andas (/home/yansym/anaconda3/lib/python3.9/site-packages)\u001b[0m\n",
      "Requirement already satisfied: sumy in /home/yansym/anaconda3/lib/python3.9/site-packages (0.10.0)\n",
      "Requirement already satisfied: requests>=2.7.0 in /home/yansym/anaconda3/lib/python3.9/site-packages (from sumy) (2.26.0)\n",
      "Requirement already satisfied: pycountry>=18.2.23 in /home/yansym/anaconda3/lib/python3.9/site-packages (from sumy) (22.3.5)\n",
      "Requirement already satisfied: breadability>=0.1.20 in /home/yansym/anaconda3/lib/python3.9/site-packages (from sumy) (0.1.20)\n",
      "Requirement already satisfied: nltk>=3.0.2 in /home/yansym/anaconda3/lib/python3.9/site-packages (from sumy) (3.5)\n",
      "Requirement already satisfied: docopt<0.7,>=0.6.1 in /home/yansym/anaconda3/lib/python3.9/site-packages (from sumy) (0.6.2)\n",
      "Requirement already satisfied: chardet in /home/yansym/anaconda3/lib/python3.9/site-packages (from breadability>=0.1.20->sumy) (4.0.0)\n",
      "Requirement already satisfied: lxml>=2.0 in /home/yansym/anaconda3/lib/python3.9/site-packages (from breadability>=0.1.20->sumy) (4.6.3)\n",
      "Requirement already satisfied: tqdm in /home/yansym/anaconda3/lib/python3.9/site-packages (from nltk>=3.0.2->sumy) (4.62.3)\n",
      "Requirement already satisfied: joblib in /home/yansym/anaconda3/lib/python3.9/site-packages (from nltk>=3.0.2->sumy) (1.1.0)\n",
      "Requirement already satisfied: regex in /home/yansym/anaconda3/lib/python3.9/site-packages (from nltk>=3.0.2->sumy) (2021.8.3)\n",
      "Requirement already satisfied: click in /home/yansym/anaconda3/lib/python3.9/site-packages (from nltk>=3.0.2->sumy) (7.1.2)\n",
      "Requirement already satisfied: setuptools in /home/yansym/anaconda3/lib/python3.9/site-packages (from pycountry>=18.2.23->sumy) (58.0.4)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/yansym/anaconda3/lib/python3.9/site-packages (from requests>=2.7.0->sumy) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/yansym/anaconda3/lib/python3.9/site-packages (from requests>=2.7.0->sumy) (3.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/yansym/anaconda3/lib/python3.9/site-packages (from requests>=2.7.0->sumy) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/yansym/anaconda3/lib/python3.9/site-packages (from requests>=2.7.0->sumy) (1.26.7)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -andas (/home/yansym/anaconda3/lib/python3.9/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -andas (/home/yansym/anaconda3/lib/python3.9/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -andas (/home/yansym/anaconda3/lib/python3.9/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -andas (/home/yansym/anaconda3/lib/python3.9/site-packages)\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "!pip install sumy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06119bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "texto = '''\n",
    "Os velejadores Beto Pandiani e Igor Bely encerraram a travessia da mítica Passagem do Noroeste em 3 de setembro de 2022. A rota de 2.500 milhas entre estreitos semicongelados teve início em 19 de julho na cidade canadense de Tuktoyaktuk, norte do Canadá, onde tiveram que esperar três semanas para o gelo se abrir. No diário de bordo do dia anterior à concluãos da travessia Beto escreveu: ‘A noite na Baia Leopold foi calma, depois de todo o aperto que passamos quando a âncora desgarrou’. Antes de dormir os dois conversaram muito sobre a derradeira navegada. Eles se sentiam inseguros em razão de previsões de tempo diferentes que receberam. Ao fim e ao cabo, decidiram-se pela travessia.\n",
    "\n",
    "\n",
    "‘Antes de dormir o Igor e eu conversamos muito a respeito da decisão de partir ou não para Artic Bay, pois tínhamos duas previsões meteorológicas bem diferentes, uma apontando um cenário favorável e outra completamente diferente.’ E o futuro mostrou que foi uma decisão acertada, apesar de terem sofrido muito frio, fome e cansaço.\n",
    "\n",
    "Antes, uma explicação sobre a Passagem do Noroeste. Dezenas de notáveis marinheiros morreram à sua procura no Ártico. No passado sua importância econômica, ao diminuir a distância entre a Europa e a Ásia, foi a chave que a fez almejada.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1a102a6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Os velejadores Beto Pandiani e Igor Bely encerraram a travessia da mítica Passagem do Noroeste em 3 de setembro de 2022. A rota de 2.500 milhas entre estreitos semicongelados teve início em 19 de julho na cidade canadense de Tuktoyaktuk, norte do Canadá, onde tiveram que esperar três semanas para o gelo se abrir. No diário de bordo do dia anterior à concluãos da travessia Beto escreveu: ‘A noite na Baia Leopold foi calma, depois de todo o aperto que passamos quando a âncora desgarrou’. Antes de dormir os dois conversaram muito sobre a derradeira navegada. Eles se sentiam inseguros em razão de previsões de tempo diferentes que receberam. Ao fim e ao cabo, decidiram-se pela travessia.\n",
      "\n",
      "\n",
      "‘Antes de dormir o Igor e eu conversamos muito a respeito da decisão de partir ou não para Artic Bay, pois tínhamos duas previsões meteorológicas bem diferentes, uma apontando um cenário favorável e outra completamente diferente.’ E o futuro mostrou que foi uma decisão acertada, apesar de terem sofrido muito frio, fome e cansaço.\n",
      "\n",
      "Antes, uma explicação sobre a Passagem do Noroeste. Dezenas de notáveis marinheiros morreram à sua procura no Ártico. No passado sua importância econômica, ao diminuir a distância entre a Europa e a Ásia, foi a chave que a fez almejada.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print (texto)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f1cbd1",
   "metadata": {},
   "source": [
    "### Frequency Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3cb8b8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sumy is a textrank based machine learning algorithm. Below is the implementation of that model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "84d1cc6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A rota de 2.500 milhas entre estreitos semicongelados teve início em 19 de julho na cidade canadense de Tuktoyaktuk, norte do Canadá, onde tiveram que esperar três semanas para o gelo se abrir.No diário de bordo do dia anterior à concluãos da travessia Beto escreveu: ‘A noite na Baia Leopold foi calma, depois de todo o aperto que passamos quando a âncora desgarrou’.‘Antes de dormir o Igor e eu conversamos muito a respeito da decisão de partir ou não para Artic Bay, pois tínhamos duas previsões meteorológicas bem diferentes, uma apontando um cenário favorável e outra completamente diferente.’ E o futuro mostrou que foi uma decisão acertada, apesar de terem sofrido muito frio, fome e cansaço.'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def solve(text):\n",
    "    stopwords1 = set(stopwords.words(\"portuguese\"))\n",
    "\n",
    "    words = word_tokenize(text)\n",
    "    freqTable = {}\n",
    "\n",
    "    for word in words:\n",
    "        word = word.lower()\n",
    "\n",
    "        if word in stopwords1:\n",
    "            continue\n",
    "        \n",
    "        if word in freqTable:\n",
    "            freqTable[word] += 1\n",
    "        else:\n",
    "            freqTable[word] = 1\n",
    "\n",
    "    sentences = sent_tokenize(text)\n",
    "    sentenceValue = {}\n",
    "    for sentence in sentences:\n",
    "        for word, freq in freqTable.items():\n",
    "            if word in sentence.lower():\n",
    "                if sentence in sentenceValue:\n",
    "                    sentenceValue[sentence] += freq\n",
    "                else:\n",
    "                    sentenceValue[sentence] = freq\n",
    "    sumValues = 0\n",
    "\n",
    "    for sentence in sentenceValue:\n",
    "        sumValues += sentenceValue[sentence]\n",
    "\n",
    "    average = int(sumValues / len(sentenceValue))\n",
    "\n",
    "    summary = ''\n",
    "    for sentence in sentences:\n",
    "        if (sentence in sentenceValue) and(sentenceValue[sentence] > (1.2 * average)):\n",
    "            summary += \"\" + sentence\n",
    "    return summary\n",
    "\n",
    "solve(texto)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6bff68d",
   "metadata": {},
   "source": [
    "### Sumy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fd5635d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is an unsupervised machine learning based approach in which we use the textrank approach to find the summary of our sentences. Using cosine similarity and vector based algorithms, we find minimum cosine distance among various words and store the more similar words together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0f4f6429",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Os velejadores Beto Pandiani e Igor Bely encerraram a travessia da mítica Passagem do Noroeste em 3 de setembro de 2022.A rota de 2.500 milhas entre estreitos semicongelados teve início em 19 de julho na cidade canadense de Tuktoyaktuk, norte do Canadá, onde tiveram que esperar três semanas para o gelo se abrir.No diário de bordo do dia anterior à concluãos da travessia Beto escreveu: ‘A noite na Baia Leopold foi calma, depois de todo o aperto que passamos quando a âncora desgarrou’.‘Antes de dormir o Igor e eu conversamos muito a respeito da decisão de partir ou não para Artic Bay, pois tínhamos duas previsões meteorológicas bem diferentes, uma apontando um cenário favorável e outra completamente diferente.’ E o futuro mostrou que foi uma decisão acertada, apesar de terem sofrido muito frio, fome e cansaço.No passado sua importância econômica, ao diminuir a distância entre a Europa e a Ásia, foi a chave que a fez almejada.\n"
     ]
    }
   ],
   "source": [
    "# Load Packages\n",
    "from sumy.parsers.plaintext import PlaintextParser\n",
    "from sumy.nlp.tokenizers import Tokenizer\n",
    "from sumy.summarizers.text_rank import TextRankSummarizer\n",
    "\n",
    "# Creating text parser using tokenization\n",
    "parser = PlaintextParser.from_string(texto, Tokenizer(\"portuguese\"))\n",
    "\n",
    "# Summarize using sumy TextRank\n",
    "summarizer = TextRankSummarizer()\n",
    "summary = summarizer(parser.document, 5)\n",
    "\n",
    "text_summary = \"\"\n",
    "for sentence in summary:\n",
    "    text_summary += str(sentence)\n",
    "\n",
    "print(text_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d36a6fb8",
   "metadata": {},
   "source": [
    "### Lex Rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4124ee90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is an unsupervised machine learning based approach in which we use the textrank approach to find the summary of our sentences. Using cosine similarity and vector based algorithms, we find minimum cosine distance among various words and store the more similar words together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "aa6bb1c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'No passado sua importância econômica, ao diminuir a distância entre a Europa e a Ásia, foi a chave que a fez almejada.'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sumy.parsers.plaintext import PlaintextParser\n",
    "from sumy.nlp.tokenizers import Tokenizer\n",
    "from sumy.summarizers.lex_rank import LexRankSummarizer\n",
    "\n",
    "\n",
    "def sumy_method(text):\n",
    "    parser = PlaintextParser.from_string(text, Tokenizer(\"portuguese\"))\n",
    "\n",
    "    summarizer = LexRankSummarizer()\n",
    "    #Summarize the document with n sentences\n",
    "    summary = summarizer(parser.document, 10)\n",
    "    dp = []\n",
    "    for i in summary:\n",
    "        lp = str(i)\n",
    "    dp.append(lp)\n",
    "    final_sentence = ' '.join(dp)\n",
    "    return final_sentence\n",
    "\n",
    "sumy_method(texto)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "828f2d9c",
   "metadata": {},
   "source": [
    "### Luhn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "22c703d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This approach is based on the frequency method; here we find TF-IDF (term frequency inverse document frequency)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e7d754ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Os velejadores Beto Pandiani e Igor Bely encerraram a travessia da mítica Passagem do Noroeste em 3 de setembro de 2022.'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sumy.summarizers.luhn import LuhnSummarizer\n",
    "\n",
    "def lunh_method(text):\n",
    "    parser = PlaintextParser.from_string(text, Tokenizer(\"english\"))\n",
    "    summarizer_luhn = LuhnSummarizer()\n",
    "    summary_1 = summarizer_luhn(parser.document, 10)\n",
    "    dp = []\n",
    "    for i in summary_1:\n",
    "        lp = str(i)\n",
    "        dp.append(lp)\n",
    "        final_sentence = ' '.join(dp)\n",
    "        return final_sentence\n",
    "    \n",
    "lunh_method(texto)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f146315",
   "metadata": {},
   "source": [
    "### LSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2a9dfe9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Latent Semantic Analyzer (LSA) is based on decomposing the data into low dimensional space. LSA has the ability to store the semantic of given text while summarizing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "35c6de1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'No passado sua importância econômica, ao diminuir a distância entre a Europa e a Ásia, foi a chave que a fez almejada.'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sumy.summarizers.lsa import LsaSummarizer\n",
    "\n",
    "def lsa_method(text):\n",
    "    parser = PlaintextParser.from_string(text, Tokenizer(\"english\"))\n",
    "    summarizer_lsa = LsaSummarizer()\n",
    "    summary_2 = summarizer_lsa(parser.document, 10)\n",
    "    dp = []\n",
    "    for i in summary_2:\n",
    "        lp = str(i)\n",
    "    dp.append(lp)\n",
    "    final_sentence = ' '.join(dp)\n",
    "    return final_sentence\n",
    "\n",
    "lsa_method(texto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc9638ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff2027b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41bbd328",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
